{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"TinyStories_all_data.tar.gz\"):\n",
    "    !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\n",
    "if not os.path.exists(\"TinyStories\"):\n",
    "    !mkdir TinyStories\n",
    "    !tar -xzf TinyStories_all_data.tar.gz -C TinyStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_filenames = sorted(glob.glob(os.path.join('TinyStories', \"*.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shard_filenames[0], \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BadEnding', 'Conflict', 'Dialogue', 'Foreshadowing', 'MoralValue', 'Twist'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set(f for x in data for f in x['instruction']['features'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = [r['story'] for r in data]\n",
    "badEndings = [r['story'] for r in data if 'BadEnding' in r['instruction']['features']]\n",
    "conflicts = [r['story'] for r in data if 'Conflict' in r['instruction']['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a girl named Lily wanted to bake a cake. She put all the things she needed on the table. Her mom helped her mix everything in a big bowl. When the cake was ready, they put it in the oven to bake.\\nWhile the cake was baking, Lily and her mom made an ornament. They made it look very pretty and attractive. They used colorful paper and shiny things. Lily said, \"Mom, this will look nice on our tree!\"\\nWhen the cake was done, they opened the oven. But, there was a surprise! The cake was not a cake anymore. It turned into a big, friendly bear! The bear said, \"Thank you for baking me! I will help you make more ornaments!\" Lily, her mom, and the bear had lots of fun making pretty things together.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text = \"\\n\".join(stories)\n",
    "badEndings_text = \"\\n\".join(badEndings)\n",
    "conflicts_text = \"\\n\".join(conflicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77586884"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\t\\n !\"$%&\\'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz|~\\xa0éñ–—‘’“”…'\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(repr(''.join(chars)))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join([itos[i] for i in ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "badEndings_data = torch.tensor(encode(badEndings_text), dtype = torch.long)\n",
    "conflicts_data = torch.tensor(encode(conflicts_text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 41, 67, 70, 83,  2, 59, 72, 62,  2, 31, 63, 72,  2, 59, 76, 63,\n",
      "         2, 64, 76, 67, 63, 72, 62, 77, 15,  2, 49, 66, 63, 83,  2, 70, 67, 69,\n",
      "        63,  2, 78, 73,  2, 74, 70, 59, 83,  2, 67, 72,  2, 78, 66, 63,  2, 74,\n",
      "        59, 76, 69, 15,  2, 44, 72, 63,  2, 62, 59, 83, 13,  2, 78, 66, 63, 83,\n",
      "         2, 77, 63, 63,  2, 59,  2, 60, 67, 65,  2, 78, 76, 63, 63,  2, 81, 67,\n",
      "        78, 66,  2, 59,  2, 77, 81, 67, 72, 65])\n",
      "'\\n\\nLily and Ben are friends. They like to play in the park. One day, they see a big tree with a swing'\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])\n",
    "print(repr(text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "badEndings_n = int(0.9*len(badEndings_data))\n",
    "badEndings_train_data = badEndings_data[:badEndings_n]\n",
    "badEndings_val_data = badEndings_data[badEndings_n:]\n",
    "\n",
    "conflicts_n = int(0.9*len(conflicts_data))\n",
    "conflicts_train_data = conflicts_data[:conflicts_n]\n",
    "conflicts_val_data = conflicts_data[conflicts_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x142454910>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [train_data, badEndings_train_data, conflicts_train_data]\n",
    "val_corpus = [val_data, badEndings_val_data, conflicts_val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, domain):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_corpus[domain] if split == 'train' else val_corpus[domain]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x142454910>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MulitHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "         nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, moe, num_experts=4):\n",
    "        super().__init__()\n",
    "        self.sa_head= MulitHeadAttention(n_head, n_embed//n_head)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_head(self.ln1(x))\n",
    "        x = x+self.ffw(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, moe=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed, device=device)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed, device=device)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head, moe=moe) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokes):\n",
    "        for _ in range(max_new_tokes):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, 0)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/5000: train loss 5.0647, val loss 5.0637\n",
      "step 100/5000: train loss 2.3834, val loss 2.3868\n",
      "step 200/5000: train loss 2.3269, val loss 2.3282\n",
      "step 300/5000: train loss 2.3055, val loss 2.3075\n",
      "step 400/5000: train loss 2.2845, val loss 2.2847\n",
      "step 500/5000: train loss 2.2615, val loss 2.2587\n",
      "step 600/5000: train loss 2.2237, val loss 2.2236\n",
      "step 700/5000: train loss 2.1778, val loss 2.1759\n",
      "step 800/5000: train loss 2.0994, val loss 2.1024\n",
      "step 900/5000: train loss 1.9945, val loss 1.9948\n",
      "step 1000/5000: train loss 1.8774, val loss 1.8794\n",
      "step 1100/5000: train loss 1.7949, val loss 1.7978\n",
      "step 1200/5000: train loss 1.7221, val loss 1.7329\n",
      "step 1300/5000: train loss 1.6702, val loss 1.6718\n",
      "step 1400/5000: train loss 1.6186, val loss 1.6233\n",
      "step 1500/5000: train loss 1.5703, val loss 1.5746\n",
      "step 1600/5000: train loss 1.5385, val loss 1.5410\n",
      "step 1700/5000: train loss 1.5080, val loss 1.5089\n",
      "step 1800/5000: train loss 1.4831, val loss 1.4842\n",
      "step 1900/5000: train loss 1.4464, val loss 1.4422\n",
      "step 2000/5000: train loss 1.4175, val loss 1.4258\n",
      "step 2100/5000: train loss 1.3959, val loss 1.4095\n",
      "step 2200/5000: train loss 1.3880, val loss 1.3794\n",
      "step 2300/5000: train loss 1.3616, val loss 1.3676\n",
      "step 2400/5000: train loss 1.3459, val loss 1.3476\n",
      "step 2500/5000: train loss 1.3315, val loss 1.3287\n",
      "step 2600/5000: train loss 1.3175, val loss 1.3178\n",
      "step 2700/5000: train loss 1.3046, val loss 1.3021\n",
      "step 2800/5000: train loss 1.2952, val loss 1.2962\n",
      "step 2900/5000: train loss 1.2712, val loss 1.2765\n",
      "step 3000/5000: train loss 1.2578, val loss 1.2569\n",
      "step 3100/5000: train loss 1.2442, val loss 1.2492\n",
      "step 3200/5000: train loss 1.2319, val loss 1.2323\n",
      "step 3300/5000: train loss 1.2216, val loss 1.2245\n",
      "step 3400/5000: train loss 1.2151, val loss 1.2172\n",
      "step 3500/5000: train loss 1.2066, val loss 1.2067\n",
      "step 3600/5000: train loss 1.1984, val loss 1.1967\n",
      "step 3700/5000: train loss 1.1890, val loss 1.1936\n",
      "step 3800/5000: train loss 1.1785, val loss 1.1819\n",
      "step 3900/5000: train loss 1.1638, val loss 1.1678\n",
      "step 4000/5000: train loss 1.1642, val loss 1.1743\n",
      "step 4100/5000: train loss 1.1557, val loss 1.1541\n",
      "step 4200/5000: train loss 1.1545, val loss 1.1514\n",
      "step 4300/5000: train loss 1.1415, val loss 1.1471\n",
      "step 4400/5000: train loss 1.1314, val loss 1.1344\n",
      "step 4500/5000: train loss 1.1368, val loss 1.1262\n",
      "step 4600/5000: train loss 1.1176, val loss 1.1300\n",
      "step 4700/5000: train loss 1.1082, val loss 1.1235\n",
      "step 4800/5000: train loss 1.1189, val loss 1.1095\n",
      "step 4900/5000: train loss 1.1053, val loss 1.1028\n",
      "step 4999/5000: train loss 1.0987, val loss 1.0965\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, domain):\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % 100 == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}/{max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train', domain)\n",
    "        xb = xb.to(device) # (batch_size, block_size)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        # compute the loss and update the model\n",
    "        logits, loss = model(xb, yb)\n",
    "        # logits shape: (batch_size * block_size, vocab_size)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train(model, optimizer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model1 = copy.deepcopy(model)\n",
    "optimizer1 = torch.optim.AdamW(model1.parameters(),lr=learning_rate)\n",
    "model2 = copy.deepcopy(model)\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/5000: train loss 1.1008, val loss 1.0971\n",
      "step 100/5000: train loss 1.1013, val loss 1.1000\n",
      "step 200/5000: train loss 1.1094, val loss 1.1012\n",
      "step 300/5000: train loss 1.0993, val loss 1.1007\n",
      "step 400/5000: train loss 1.1002, val loss 1.0983\n",
      "step 500/5000: train loss 1.0967, val loss 1.0961\n",
      "step 600/5000: train loss 1.0971, val loss 1.0986\n",
      "step 700/5000: train loss 1.0995, val loss 1.1041\n",
      "step 800/5000: train loss 1.1012, val loss 1.0965\n",
      "step 900/5000: train loss 1.1043, val loss 1.1004\n",
      "step 1000/5000: train loss 1.0905, val loss 1.1004\n",
      "step 1100/5000: train loss 1.1003, val loss 1.1028\n",
      "step 1200/5000: train loss 1.1006, val loss 1.0981\n",
      "step 1300/5000: train loss 1.0959, val loss 1.1004\n",
      "step 1400/5000: train loss 1.0963, val loss 1.0987\n",
      "step 1500/5000: train loss 1.1000, val loss 1.1020\n",
      "step 1600/5000: train loss 1.0954, val loss 1.0964\n",
      "step 1700/5000: train loss 1.1009, val loss 1.1018\n",
      "step 1800/5000: train loss 1.0926, val loss 1.1044\n",
      "step 1900/5000: train loss 1.0981, val loss 1.0998\n",
      "step 2000/5000: train loss 1.1020, val loss 1.1054\n",
      "step 2100/5000: train loss 1.1004, val loss 1.1032\n",
      "step 2200/5000: train loss 1.1013, val loss 1.1099\n",
      "step 2300/5000: train loss 1.1053, val loss 1.1034\n",
      "step 2400/5000: train loss 1.0996, val loss 1.0988\n",
      "step 2500/5000: train loss 1.0983, val loss 1.0948\n",
      "step 2600/5000: train loss 1.0924, val loss 1.1045\n",
      "step 2700/5000: train loss 1.1019, val loss 1.1045\n",
      "step 2800/5000: train loss 1.1038, val loss 1.1008\n",
      "step 2900/5000: train loss 1.0992, val loss 1.0988\n",
      "step 3000/5000: train loss 1.0936, val loss 1.1068\n",
      "step 3100/5000: train loss 1.0999, val loss 1.0936\n",
      "step 3200/5000: train loss 1.0958, val loss 1.1007\n",
      "step 3300/5000: train loss 1.1006, val loss 1.1013\n",
      "step 3400/5000: train loss 1.1042, val loss 1.0957\n",
      "step 3500/5000: train loss 1.0951, val loss 1.0923\n",
      "step 3600/5000: train loss 1.0979, val loss 1.1002\n",
      "step 3700/5000: train loss 1.1016, val loss 1.1011\n",
      "step 3800/5000: train loss 1.0983, val loss 1.1045\n",
      "step 3900/5000: train loss 1.1003, val loss 1.0964\n",
      "step 4000/5000: train loss 1.0929, val loss 1.1008\n",
      "step 4100/5000: train loss 1.1049, val loss 1.1025\n",
      "step 4200/5000: train loss 1.0925, val loss 1.0931\n",
      "step 4300/5000: train loss 1.0964, val loss 1.0978\n",
      "step 4400/5000: train loss 1.1054, val loss 1.1028\n",
      "step 4500/5000: train loss 1.0994, val loss 1.1013\n",
      "step 4600/5000: train loss 1.1026, val loss 1.0976\n",
      "step 4700/5000: train loss 1.0965, val loss 1.1024\n",
      "step 4800/5000: train loss 1.1012, val loss 1.1021\n",
      "step 4900/5000: train loss 1.0992, val loss 1.1018\n",
      "step 4999/5000: train loss 1.0970, val loss 1.0974\n"
     ]
    }
   ],
   "source": [
    "train(model1, optimizer1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/5000: train loss 1.1024, val loss 1.0976\n",
      "step 100/5000: train loss 1.1028, val loss 1.0981\n",
      "step 200/5000: train loss 1.0919, val loss 1.1012\n",
      "step 300/5000: train loss 1.0911, val loss 1.1030\n",
      "step 400/5000: train loss 1.0985, val loss 1.0986\n",
      "step 500/5000: train loss 1.1037, val loss 1.1061\n",
      "step 600/5000: train loss 1.0991, val loss 1.0978\n",
      "step 700/5000: train loss 1.1056, val loss 1.1010\n",
      "step 800/5000: train loss 1.0994, val loss 1.0986\n",
      "step 900/5000: train loss 1.0922, val loss 1.0990\n",
      "step 1000/5000: train loss 1.0953, val loss 1.0973\n",
      "step 1100/5000: train loss 1.0988, val loss 1.0995\n",
      "step 1200/5000: train loss 1.0986, val loss 1.1028\n",
      "step 1300/5000: train loss 1.0966, val loss 1.0982\n",
      "step 1400/5000: train loss 1.0945, val loss 1.0984\n",
      "step 1500/5000: train loss 1.0988, val loss 1.1010\n",
      "step 1600/5000: train loss 1.0965, val loss 1.1071\n",
      "step 1700/5000: train loss 1.0919, val loss 1.1042\n",
      "step 1800/5000: train loss 1.0979, val loss 1.1041\n",
      "step 1900/5000: train loss 1.0958, val loss 1.0996\n",
      "step 2000/5000: train loss 1.0892, val loss 1.0994\n",
      "step 2100/5000: train loss 1.0993, val loss 1.1029\n",
      "step 2200/5000: train loss 1.0925, val loss 1.0979\n",
      "step 2300/5000: train loss 1.1032, val loss 1.1061\n",
      "step 2400/5000: train loss 1.0916, val loss 1.1001\n",
      "step 2500/5000: train loss 1.0969, val loss 1.1079\n",
      "step 2600/5000: train loss 1.0975, val loss 1.1069\n",
      "step 2700/5000: train loss 1.0956, val loss 1.1001\n",
      "step 2800/5000: train loss 1.0980, val loss 1.1052\n",
      "step 2900/5000: train loss 1.0981, val loss 1.0956\n",
      "step 3000/5000: train loss 1.0966, val loss 1.0947\n",
      "step 3100/5000: train loss 1.0970, val loss 1.0954\n",
      "step 3200/5000: train loss 1.1028, val loss 1.1024\n",
      "step 3300/5000: train loss 1.0999, val loss 1.0964\n",
      "step 3400/5000: train loss 1.0938, val loss 1.1042\n",
      "step 3500/5000: train loss 1.0963, val loss 1.0979\n",
      "step 3600/5000: train loss 1.0931, val loss 1.1033\n",
      "step 3700/5000: train loss 1.0971, val loss 1.0995\n",
      "step 3800/5000: train loss 1.0947, val loss 1.1031\n",
      "step 3900/5000: train loss 1.1032, val loss 1.1004\n",
      "step 4000/5000: train loss 1.0982, val loss 1.1010\n",
      "step 4100/5000: train loss 1.1003, val loss 1.1061\n",
      "step 4200/5000: train loss 1.0972, val loss 1.0985\n",
      "step 4300/5000: train loss 1.0973, val loss 1.1072\n",
      "step 4400/5000: train loss 1.1010, val loss 1.1004\n",
      "step 4500/5000: train loss 1.0992, val loss 1.1020\n",
      "step 4600/5000: train loss 1.0945, val loss 1.1021\n",
      "step 4700/5000: train loss 1.1001, val loss 1.0989\n",
      "step 4800/5000: train loss 1.0971, val loss 1.0941\n",
      "step 4900/5000: train loss 1.0930, val loss 1.0935\n",
      "step 4999/5000: train loss 1.0951, val loss 1.1025\n"
     ]
    }
   ],
   "source": [
    "train(model2, optimizer2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = torch.ones(2, device=device) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(expert1, expert2, x, prior):\n",
    "    # prior shape: [2]\n",
    "    # x shape: [1, seq_len]\n",
    "    _, loss1 = expert1(x, x)  # shape: a number\n",
    "    _, loss2 = expert2(x, x)\n",
    "    logPost = torch.stack([-loss1, -loss2], dim=0) + torch.log(prior)  # shape: [2]\n",
    "    post = torch.exp(logPost)  # shape: [2]\n",
    "    post = post / post.sum(dim=0)\n",
    "    return post  # shape: [2]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(expert1, expert2, x, max_new_tokes):\n",
    "    global prior\n",
    "    expert1.eval()\n",
    "    expert2.eval()\n",
    "    expert_probs_all = []\n",
    "    for _ in range(max_new_tokes):\n",
    "        xb = x[:, -block_size:]\n",
    "        logits1, _ = expert1(xb)\n",
    "        logits1 = logits1[:, -1, :]\n",
    "        probs1 = F.softmax(logits1, dim=-1)  # shape: [batch_size, vocab_size]\n",
    "\n",
    "        logits2, _ = expert1(xb)\n",
    "        logits2 = logits2[:, -1, :]\n",
    "        probs2 = F.softmax(logits2, dim=-1)\n",
    "\n",
    "        post = posterior(expert1, expert2, xb, prior)  # shape: [batch_size, 2]\n",
    "        expert_probs_all.append(post)\n",
    "\n",
    "        # print(probs1.shape, post.shape, post[0].shape)\n",
    "        probs = probs1 * post[0] + probs2 * post[1]\n",
    "\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat((x, idx_next), dim=1)\n",
    "\n",
    "    # update prior using exponential moving average\n",
    "    prior = (\n",
    "        pd.DataFrame(expert_probs_all)\n",
    "        .ewm(alpha=0.3, adjust=False)\n",
    "        .mean()\n",
    "        .tail(n=1)\n",
    "        .to_numpy()\n",
    "        .squeeze(0)\n",
    "    )\n",
    "    prior = torch.Tensor(prior).to(device)\n",
    "    expert1.train()\n",
    "    expert2.train()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time there was a box trunning and ran him. She was very sad that she could not like the room what he wanted her to go to the hospital. She felt sad and cried. Her mom had mean with her mommy and firsts bed. The end.\n",
      "Once upon a time, there was a little girl named Lily. She had a necklace and showed the tree. She acidentally not put herself. Fred was still and jumpsterful ent. Her promised everywore with his branchle tomaties. \n",
      "One day, the cat rashed wagon and wanted to joursely. He laughing so the towels and ma\n"
     ]
    }
   ],
   "source": [
    "d = 'once upon a time there was a '\n",
    "x = torch.tensor(encode(d), dtype = torch.long,device=device).unsqueeze(0)\n",
    "print(decode(generate(model1, model2, x, max_new_tokes=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, Lily went to the park. He took her walked ap number cameroas to always with Nerow. The sky ashtrayed to give it a big, small, but it made a large big him and friends. Ony, she found his mommy took him, she found a girl and wanted to see the seathion. They were sad and leaves to try that's went too parents again.\n",
      "But went Anna's higin-y toys, he started to dig became high tictors. He didn't like the kulon books, but no being careful around you share.\n",
      "At was was sad, but enving his trainy hand s\n"
     ]
    }
   ],
   "source": [
    "d = 'One day'\n",
    "x = torch.tensor(encode(d), dtype = torch.long,device=device).unsqueeze(0)\n",
    "print(decode(generate(model1, model2, x, max_new_tokes=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4280, 0.5720])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
